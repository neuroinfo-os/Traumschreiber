{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "#from ipynb.fs.full.AppProcessing import *\n",
    "from itertools import zip_longest\n",
    "import itertools as itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareCsvForPkgId(input_file, returnjustfile = False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        file (string): Is a csv produced by the app after 11.09.2020 containing info on the package ID. \n",
    "        returnjustfile (boolean = False): If true, only the file and not the list with Marc's ids, the filename and  the evenly index as list is returned\n",
    "    Returns:\n",
    "        file (dataframe): A kind of cleaned dataframe with an evenly space timestamp as index \n",
    "        ids (list): A list with the package ids as ints\n",
    "        filename (string): The name of the experiment (if the name given from the app, including the timestamp) was used)\n",
    "        index_evenly (array): The evenly spaced index as a list\n",
    "    \"\"\"\n",
    "    file = pd.read_csv(input_file)\n",
    "    file = file.iloc[1:]\n",
    "    file.columns = file.iloc[0]\n",
    "\n",
    "    file = file[1:].apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    # to evenly set the timestamps \n",
    "    length = len(file.index) # length of dataframe\n",
    "    stop = length*4 # stop point \n",
    "    index_evenly = np.linspace(0, stop, num = length, endpoint = False) # to adapt timestamp  \n",
    "    file.loc[:,\"Time\"] = index_evenly #set the time as index, relevant for later plotting\n",
    "\n",
    "    ids= file.index.tolist() # these are all the package ids (as strings because, yes pandas of course we want that as strings, strings are soooo useful..Not)\n",
    "    ids = [int(one_id) for one_id in ids] # now they are ints\n",
    "    #print(ids)\n",
    "\n",
    "    file.set_index('Time', inplace=True)\n",
    "    # \n",
    "    filename = input_file[0:6]+input_file[20:-4]\n",
    "    if returnjustfile:\n",
    "        return file\n",
    "    else:\n",
    "        return file, ids, filename, index_evenly # no idea why, but the name of the index still includes \"Pkg ID\", the id not actually the index. Time is .. \n",
    "    \n",
    "\n",
    "def PlotJumpsBoth(justTheFileName,  which =\"both\", save = False):\n",
    "    \"\"\"\n",
    "    Plots the parts of the Signal where the difference to the next measurement is larger than 3.7, indicating an abnormalty\n",
    "    Input:\n",
    "        justTheFileName (string): The name of the csv to be processed. has to be in same folder. \n",
    "    \"\"\"\n",
    "    name = justTheFileName[0:6]+justTheFileName[20:-4]\n",
    "    \n",
    "    withLoss = prepareCsvForPkgId(justTheFileName, returnjustfile = True).drop([\"Ch-2\",\"Ch-3\",\"Ch-4\",\"Ch-5\",\"Ch-6\",\"Ch-7\",\"Ch-8\"], axis=1).dropna(axis=1) \n",
    "\n",
    "    dif_plot = withLoss.drop([\"Pkg Loss\"], axis=1) \n",
    "\n",
    "#the df with the difference to the next measurement and Channel 1 info\n",
    "    dif_plot[\"Difference\"] =dif_plot.diff(axis=0).abs()\n",
    "\n",
    "    # now really only the difference\n",
    "    dif_df = dif_plot.drop([\"Ch-1\"], axis =1)\n",
    "\n",
    "    # reduce to the ones larger than threshold\n",
    "    bigger = dif_df[(dif_df >3.7).any(1)]\n",
    "\n",
    "    # only look at 100 largest difference \n",
    "    biggest_differences_df = bigger[\"Difference\"].nlargest(n=100)\n",
    "    #get the timestamp of the biggest difference\n",
    "    bd_list = biggest_differences_df.index.tolist() #list of the timestamp when the biggest difference occur\n",
    "    #sort the list of timestamps to group them (so that they can be plotted more efficiently )\n",
    "    sortedList = sorted(bd_list)\n",
    "\n",
    "    # create dictionary of groups of big differences \n",
    "    DictOfJumps = dict(enumerate(grouper2(sortedList, closeMeansWithin=100), 1))\n",
    "    ratio = round( len(bigger.index)/len(dif_plot.index),5)\n",
    "    print(\"For ~{}~ and its {} measured data points, {} have an bigger than threshold difference. The ratio is {}. The jumps occur in {} parts of the signal\".format(name,len(dif_plot.index),len(bigger.index),ratio,len(DictOfJumps)))\n",
    "\n",
    "    # combine the \"lost package\" info with the  difference info for convenient plotting \n",
    "    combined = pd.concat([withLoss, dif_df], axis=1)\n",
    "    if which == \"both\" or \"difference\": \n",
    "        for key, value in DictOfJumps.items():\n",
    "            first_jump = value[0]\n",
    "            first_jump_pos = int(first_jump/4)\n",
    "            last_jump = value[-1]\n",
    "            last_jump_pos = int(last_jump/4)\n",
    "            plot_from = first_jump_pos -45\n",
    "            if plot_from < 0:\n",
    "                plot_from = 0\n",
    "            plot_to = last_jump_pos +45\n",
    "            if plot_to > len(dif_plot.index):\n",
    "                plot_to = len(dif_plot.index)\n",
    "            rangePlot = plot_to - plot_from\n",
    "            adaptsize = int(rangePlot/10)\n",
    "            combined.iloc[plot_from:plot_to].plot(marker ='o', title = \"{}:There are {} large differences between {} and {} msec\".format(name,len(value),first_jump, last_jump),figsize = (adaptsize,5), grid=True)\n",
    "            if save:\n",
    "                plt.savefig(name+\"_\"+str(first_jump)+\"_to_\"+str(last_jump)+'_msec.png')\n",
    "\n",
    "        #dif_df.plot(title = \"Is there a pattern to where the signal jumps?\", figsize = (15,5))\n",
    "    \n",
    "    if which == \"both\" or \"lost\":\n",
    "        \n",
    "        #dif_plot = prepareCsvForPkgId(justTheFile, returnjustfile = True).drop([\"Ch-2\",\"Ch-3\",\"Ch-4\",\"Ch-5\",\"Ch-6\",\"Ch-7\",\"Ch-8\"], axis=1).dropna(axis=1) \n",
    "\n",
    "        #dif_plot[\"Difference\"] =dif_plot.diff(axis=0).abs()\n",
    "        dif_df = withLoss.drop([\"Ch-1\"], axis =1)\n",
    "        # find place where the package loss is bigger than 0 \n",
    "        bigger = dif_df[(dif_df >0).any(1)]\n",
    "\n",
    "        bd_list = bigger.index.tolist()\n",
    "        sortedList = sorted(bd_list) # list of the indexed with jumps\n",
    "\n",
    "        DictOfLosses = dict(enumerate(grouper2(sortedList, closeMeansWithin=100), 1))\n",
    "        ratio = round( len(bigger.index)/len(dif_plot.index),5)\n",
    "        total = sum(dif_df[\"Pkg Loss\"])\n",
    "        print(\"For ~{}~ and its {} measured data points, {} are lost in transmission. The ratio is {}. \\n The losses occur in {} parts of the signal. \".format(name,len(dif_plot.index), total, ratio, len(DictOfJumps)))\n",
    "\n",
    "        for key, value in DictOfLosses.items():\n",
    "\n",
    "            first_jump = value[0]\n",
    "            first_jump_pos = int(first_jump/4)\n",
    "            last_jump = value[-1]\n",
    "            last_jump_pos = int(last_jump/4)\n",
    "            plot_from = first_jump_pos -20\n",
    "            if plot_from < 0:\n",
    "                plot_from = 0\n",
    "            plot_to = last_jump_pos +20\n",
    "            if plot_to > len(dif_plot.index):\n",
    "                plot_to = len(dif_plot.index)\n",
    "            rangePlot = plot_to - plot_from\n",
    "            adaptsize = int(rangePlot/10)\n",
    "            combined.iloc[plot_from:plot_to].plot(marker ='o', title = \"{}:There are {} packages lost in transmission between {} and {} msec\".format(name,len(value),first_jump, last_jump), grid=True , figsize = (adaptsize,5))\n",
    "            if save:\n",
    "                plt.savefig(name+\"_Packageloss_\"+str(first_jump)+\"_to_\"+str(last_jump)+'_msec.png')     \n",
    "        #dif_df.plot(title = \"Is there a pattern in the timing of lost packages?\",figsize = (15,5))\n",
    "def grouper2(iterable, closeMeansWithin = 50): # from stackoverflow\n",
    "    \"\"\"\n",
    "    Helper function to evaluate if drops and jumps are in the same area\n",
    "    \"\"\"\n",
    "    prev = None\n",
    "    group = []\n",
    "    for item in iterable:\n",
    "        if not prev or item - prev <= closeMeansWithin: # if the values are in a range of 50, they are in the same cycle (approximately)\n",
    "            group.append(item)\n",
    "        else:\n",
    "            yield group\n",
    "            group = [item]\n",
    "        prev = item\n",
    "    if group:\n",
    "        yield group\n",
    "\n",
    "result_dict = {\"Name\": [], \"Total\": [], \"Complete\": [], \"Ratio\":[]} # to later save the results\n",
    "result_df = pd.DataFrame(data=result_dict)\n",
    "\n",
    "def evalCompleteness(IdChunkList, filename, perfectSet =  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}):\n",
    "    '''\n",
    "    Checks whether the individual chunks contain all package ids and are hence a complete set where nothing was dropped.\n",
    "    Prints out a summary and saves the results in a dataframe (result_df)\n",
    "    Input:\n",
    "        chunklist(list of list): The list created by grouper\n",
    "        filename (string): The name of the experiment, as produced by prepareCsvForPkgId() \n",
    "        perfectSet (set): The set of values that comprise all possible package ids, given by the app. \n",
    "                        Older Csv files contain the old naming ({1, 2, 3, 4, 5, 6, 7, -8, -7, -6, -5, -4, -3, -2, -1, 0}),\n",
    "                        Newer ones, the default one ({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15})\n",
    "    '''\n",
    "    ratio = []\n",
    "    for chunk in IdChunkList:\n",
    "        isSet = perfectSet.issubset(set(chunk))\n",
    "        ratio.append(isSet)\n",
    "    total = len(ratio)-1 # -1 because the last one usually is never complete\n",
    "    complete = sum(ratio)\n",
    "    ratio_abs = complete/total\n",
    "    \n",
    "    if ratio_abs < 0.1: \n",
    "        ratio = []\n",
    "        print(\"Low results of ~{}~ indicate the old encoding scheme:\".format(filename))\n",
    "        perfectSet = {1, 2, 3, 4, 5, 6, 7, -8, -7, -6, -5, -4, -3, -2, -1, 0}\n",
    "        for chunk in IdChunkList:\n",
    "            isSet = perfectSet.issubset(set(chunk))\n",
    "            ratio.append(isSet)\n",
    "        total = len(ratio)-1 # -1 because the last one usually is never complete\n",
    "        complete = sum(ratio)\n",
    "        ratio_abs = complete/total\n",
    "    print(\"Of {} chunks, {} contain all package Ids. The complete to incomplete ratio of the experiment ~{}~ is: {}.\".format(total, complete, filename, ratio_abs))\n",
    "    global result_df\n",
    "    result_df = result_df.append({\"Name\": filename, \"Total\": total, \"Complete\": complete, \"Ratio\": ratio_abs}, ignore_index= True, verify_integrity=True)\n",
    "    result_df.drop_duplicates(inplace=True) # clean that df\n",
    "    \n",
    "def analyseMissingAppPackages(filestring, perfectSet = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}):\n",
    "    '''\n",
    "    Function combines all steps of analysing missing packages that are dropped by the app  \n",
    "    Input:\n",
    "        filestring (string): The name of the file to be analysed, has to be in same directory\n",
    "        perfectSet (set): The set of values that comprise all possible package ids, given by the app. \n",
    "                        Older Csv files contain the old naming ({1, 2, 3, 4, 5, 6, 7, -8, -7, -6, -5, -4, -3, -2, -1, 0}),\n",
    "                        Newer ones, the default one ({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15})\n",
    "    '''\n",
    "    \n",
    "    file_df, ids_list, filename_string, index = prepareCsvForPkgId(filestring)\n",
    "    try:\n",
    "        print(\"\\n Package Loss For\", filename_string, \"~ \",sum(file_df[\"Pkg Loss\"].tolist()), \" ~\", \"Total Amount of datapoints: \", len(file_df.index))\n",
    "        print(\"Adapted (/3 due to BL characteristics) Percentage = \", ((sum(file_df[\"Pkg Loss\"].tolist())/len(file_df.index))*100)/3)\n",
    "    except KeyError:\n",
    "        print(\" \")\n",
    "        #print(\"\\n no Pkg Loss available for \", filename_string)\n",
    "    IdChunkList = grouper(ids_list, 16, 20) # 20 is just a random number bigger than the largest value in set (to fill the last set )\n",
    "    evalCompleteness(IdChunkList, filename_string, perfectSet)\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None): # copied from StackOverflow\n",
    "    '''\n",
    "    Mostly Copied from StackOverflow. Takes a list and devided it into chunks of length n.\n",
    "    Returns:\n",
    "        toReturn: A list containing lists of length n\n",
    "    '''\n",
    "    args = [iter(iterable)] * n\n",
    "    toReturn = list(zip_longest(*args, fillvalue=fillvalue)) # includes now the data types I want to work with (namely list of lists)\n",
    "    toReturn = [list(oneChunk) for oneChunk in toReturn] # because one cannot turn tuples into sets. \n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 25-09-2020_12-37-13_30min_Motorola.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkrie\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3325: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Package Loss For 25-09-30min_Motorola ~  876  ~ Total Amount of datapoints:  325031\n",
      "Adapted (/3 due to BL characteristics) Percentage =  0.08983758472268799\n",
      "Of 20314 chunks, 20314 contain all package Ids. The complete to incomplete ratio of the experiment ~25-09-30min_Motorola~ is: 1.0.\n",
      "\n",
      " 25-09-2020_13-10-58_30min_Honor.csv\n",
      "\n",
      " Package Loss For 25-09-30min_Honor ~  351  ~ Total Amount of datapoints:  468486\n",
      "Adapted (/3 due to BL characteristics) Percentage =  0.02497406539362969\n",
      "Of 29280 chunks, 29280 contain all package Ids. The complete to incomplete ratio of the experiment ~25-09-30min_Honor~ is: 1.0.\n",
      "\n",
      " 25-09-2020_16-18-14_motorola_2.csv\n",
      "\n",
      " Package Loss For 25-09-motorola_2 ~  214  ~ Total Amount of datapoints:  232168\n",
      "Adapted (/3 due to BL characteristics) Percentage =  0.030724877387638835\n",
      "Of 14510 chunks, 14510 contain all package Ids. The complete to incomplete ratio of the experiment ~25-09-motorola_2~ is: 1.0.\n",
      "\n",
      " 25-09-2020_16-33-28_15min_HonorWithWifi.csv\n",
      "\n",
      " Package Loss For 25-09-15min_HonorWithWifi ~  80153  ~ Total Amount of datapoints:  128745\n",
      "Adapted (/3 due to BL characteristics) Percentage =  20.752391678641242\n",
      "Of 8046 chunks, 8046 contain all package Ids. The complete to incomplete ratio of the experiment ~25-09-15min_HonorWithWifi~ is: 1.0.\n"
     ]
    }
   ],
   "source": [
    "extension = 'csv'\n",
    "all_csv = glob.glob('*.{}'.format(extension))\n",
    "for each_file in all_csv:\n",
    "    \n",
    "    print(\"\\n\", each_file)\n",
    "    #df = prepareCsvForPkgId(each_file, returnjustfile = True)\n",
    "    #print(df)\n",
    "    #print(\"Length:\",len(df.index),\"\\n Sum:\", sum(df[\"Pkg Loss\"]), \"\\n Ratio:\", sum(df[\"Pkg Loss\"])/len(df.index))\n",
    "    #PlotJumpsBoth(each_file, which = \"difference\", save = True)\n",
    "    analyseMissingAppPackages(each_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
